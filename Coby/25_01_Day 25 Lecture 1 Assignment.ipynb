{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day 25 Lecture 1 Assignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2-final"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kOVPVzs51Zbk"},"source":["## Day 25 Lecture 1 Assignment\n","\n","In this assignment, we will evaluate the performance of the model we built yesterday on the Chicago traffic crash data. We will also perform hyperparameter tuning and evaluate a final model using additional metrics (e.g. AUC-ROC, precision, recall, etc.)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import ds_useful"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UkPoFJCueQZ3"},"source":["Since we will be building on the model we built in the last assignment, we will need to redo all of the data preparation steps up to the point of model building. These steps include creating the response, missing value imputation, and one-hot encoding our selected categorical variables. The quickest way to get going would be to open last week's assignment, make a copy, and build on it from there."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"target:\n damage\n\n\nfeatures:\n"},{"data":{"text/plain":"Index(['weather_condition_CLOUDY/OVERCAST', 'weather_condition_FOG/SMOKE/HAZE',\n       'weather_condition_FREEZING RAIN/DRIZZLE', 'weather_condition_OTHER',\n       'weather_condition_RAIN', 'weather_condition_SEVERE CROSS WIND GATE',\n       'weather_condition_SLEET/HAIL', 'weather_condition_SNOW',\n       'weather_condition_UNKNOWN', 'first_crash_type_ANIMAL',\n       'first_crash_type_FIXED OBJECT', 'first_crash_type_HEAD ON',\n       'first_crash_type_OTHER NONCOLLISION', 'first_crash_type_OTHER OBJECT',\n       'first_crash_type_OVERTURNED', 'first_crash_type_PARKED MOTOR VEHICLE',\n       'first_crash_type_PEDALCYCLIST', 'first_crash_type_PEDESTRIAN',\n       'first_crash_type_REAR TO FRONT', 'first_crash_type_REAR TO REAR',\n       'first_crash_type_REAR TO SIDE',\n       'first_crash_type_SIDESWIPE OPPOSITE DIRECTION',\n       'first_crash_type_SIDESWIPE SAME DIRECTION', 'first_crash_type_TRAIN',\n       'first_crash_type_TURNING', 'posted_speed_limit', 'injuries_total'],\n      dtype='object')"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Load the dataset\n","crash_data = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/traffic_crashes_chicago.csv')\n","\n","#Rename the columns for easy reference\n","crash_data.rename(columns=lambda x: x.lower(), inplace=True)\n","\n","# create a binary response column by modifying the \"DAMAGE\" column. Consider \"OVER \\$1500\" to be the positive class, and under \\$1500 to be the negative class.\n","crash_data.damage = np.where(crash_data.damage == 'OVER $1,500', 1, 0)\n","\n","# Dropping all columns with more than 5% missing data\n","    # Get missing data\n","missing = ds_useful.missingness_summary(crash_data)\n","    # Drop the indexes of missing whose values are greater than 5\n","crash_data.drop(columns=missing.loc[missing > 5].index, axis=1, inplace=True)\n","\n","# Imputate all numeric (implied) data with the linear model\n","crash_data = crash_data.interpolate(method='linear')\n","\n","# Drop any null categorical values\n","for col in crash_data.select_dtypes('object').columns:\n","    crash_data.drop(crash_data.loc[crash_data[col].isnull()].index, axis='rows', inplace=True)\n","\n","# Define raw feature variables from attributes\n","keep = ['posted_speed_limit', 'weather_condition', 'injuries_total', 'first_crash_type', 'damage']\n","keep_df = crash_data[keep]\n","\n","# Identify the highest occuring variable for each. We'll drop these to help dimensionality.\n","keep_df.drop(keep_df.loc[keep_df['weather_condition'] == 'CLEAR'].index, inplace=True)\n","keep_df.drop(keep_df.loc[keep_df['first_crash_type'] == 'REAR END'].index, inplace=True)\n","\n","# Now let's one hot encode the categorical data.\n","feature_df = pd.get_dummies(keep_df.select_dtypes('object'), drop_first=True)\n","\n","# Then we'll add our categorical data to the feature dataframe\n","feature_df['posted_speed_limit'] = keep_df['posted_speed_limit']\n","feature_df['injuries_total'] = keep_df['injuries_total']\n","\n","# Finally, we'll identify our target label and create X and y groups\n","target = 'damage'\n","X = feature_df\n","Y = keep_df[target]\n","\n","print('target:\\n', target)\n","print('\\n\\nfeatures:')\n","X.columns"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P7hI35MY1Zbq"},"source":["Statsmodels' implementation of logistic has certain advantages over scikit-learn's, such as clean, easy to read model summary output and statistical inference values (e.g. p-values). However, scikit-learn is preferable for model evaluation, so we will switch to the scikit-learn implementation for this exercise. \n","\n","Run logistic regression on the training set and use the resulting model to make predictions on the test set. Calculate the train and test error using logarithmic loss. How do they compare to each other?"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Training Set Score 0.6246670809659091\nTest Set Score     0.6247669773635153\nTraining Set Error 0.6388141672799862\nTest Set Error     0.6429140912598977\n\nThe error for the test set is slightly larger than the error for the training set. This leads me to believe that there may be a slight amount of overfitting. However, in a previous assignment, I actually optimized this log_loss error to choose hyperparameters, so this is about as good as this will get.\n"}],"source":["# answer goes here\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss\n","\n","# Create Training and Testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=1)\n","\n","# Initiate the lr object, using the .1 C-value, determined from the other assignment\n","lr = LogisticRegression(C=.1, penalty='l2', solver='lbfgs')\n","# Train it with the training data\n","lr.fit(X_train, Y_train)\n","\n","# Make probabalistic predictions on the test and train sets\n","train_prob_pred = lr.predict_proba(X_train)\n","test_prob_pred = lr.predict_proba(X_test)\n","\n","print('Training Set Score', lr.score(X_train, Y_train))\n","print('Test Set Score    ', lr.score(X_test, Y_test))\n","\n","# Use proba predictions to calculate the log loss\n","train_loss = log_loss(Y_train, train_prob_pred)\n","test_loss = log_loss(Y_test, test_prob_pred)\n","\n","print('Training Set Error', train_loss)\n","print('Test Set Error    ', test_loss)\n","\n","print('\\nThe error for the test set is slightly larger than the error for the training set. This leads me to believe that there may be a slight amount of overfitting. However, in a previous assignment, I actually optimized this log_loss error to choose hyperparameters, so this is about as good as this will get.')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"27_OUrv11Zbt"},"source":["Next, evaluate the performance of the same model using 10-fold CV. Use the training data and labels, and print out the mean log loss for each of the 10 CV folds, as well as the overall CV-estimated test error. How do the estimates from the individual folds compare to the result from our previous single holdout set? How much variability in the estimated test error do you see across the 10 folds?\n","\n","Note: scikit-learn's *cross_val_score* function provides a simple, one-line method for doing this. However, be careful - the default score returned by this function may not be log loss!"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy for the 10 folds: [0.6298269  0.62827341 0.62494452 0.62472259 0.62072792 0.626498\n 0.61775805 0.62552719 0.61931188 0.62130966]\nMean accuracy from folds: 0.623890011246566\n"}],"source":["# answer goes here\n","from sklearn.model_selection import cross_validate, cross_val_score\n","cv_scores = cross_val_score(lr, X_train, Y_train, cv=10)\n","print('Accuracy for the 10 folds:', cv_scores)\n","print('Mean accuracy from folds:', np.mean(cv_scores))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K_Kp_W-P1Zbv"},"source":["Scikit-learn's logistic regression function has a built-in regularization parameter, C (the larger the value of C, the smaller the degree of regularization). Use cross-validation and grid search to find an optimal value for the parameter C."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":"{'fit_time': array([0.6033926 , 0.81752968, 0.70871401, 0.85485864, 0.66452932,\n        0.74509382, 0.5885222 , 0.69581842, 0.62267995, 0.61730742]),\n 'score_time': array([0.00200272, 0.00398803, 0.00298858, 0.00299168, 0.00299597,\n        0.00398827, 0.0039959 , 0.00597906, 0.00398898, 0.00399208]),\n 'test_score': array([0.6298269 , 0.62827341, 0.62494452, 0.62472259, 0.62072792,\n        0.626498  , 0.61775805, 0.62552719, 0.61931188, 0.62130966])}"},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# answer goes here\n","\n","cross_validate(lr, X_train, Y_train, cv=10)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eJ-5cPUp1Zbx"},"source":["Re-train a logistic regression model using the best value of C identified by 10-fold CV on the training data and labels. Afterwards, do the following:\n","\n","- Determine the precision, recall, and F1-score of our model using a cutoff/threshold of 0.5 (hint: scikit-learn's *classification_report* function may be helpful)\n","- Plot or otherwise generate a confusion matrix\n","- Plot the ROC curve for our logistic regression model\n","\n","Note: the performance of our simple logistic regression model with just four features will not be very good, but this is not entirely unexpected. There are many other features that can be incorporated into the model to improve its performance; feel free to experiment!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"v3A9fUN51Zbx"},"outputs":[],"source":["# answer goes here\n","\n","\n","\n"]}]}