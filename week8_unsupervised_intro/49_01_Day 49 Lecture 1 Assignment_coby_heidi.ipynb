{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day 49 Lecture 1 Assignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2-final"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xdL0Yz0D1D_q"},"source":["## Day 49 Lecture 1 Assignment\n","\n","In this assignment, we will apply GMM (Gaussian Mixture Modeling) clustering to a dataset containing player-season statistics for NBA players from the past four years."]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.mixture import GaussianMixture\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from scipy.special import entr"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ezMvoRlB1D_t"},"source":["This dataset contains player-season statistics for NBA players from the past four years. Each row in this dataset represents a player's per-game averages for a single season. \n","\n","This dataset contains the following variables:\n","\n","- Seas: season ('2019' = 2018-2019 season, '2018' = 2017-2018 season, etc.)\n","- Player: player name\n","- Pos: position\n","- Age: age\n","- Tm: team\n","- G: games played\n","- GS: games started\n","- MP: minutes played\n","- FG: field goals\n","- FGA: field goals attempted\n","- FG%: field goal percentage\n","- 3P: 3 pointers\n","- 3PA: 3 pointers attempted\n","- 3P%: 3 point percentage\n","- 2P: 2 pointers\n","- 2PA: 2 pointers attempted\n","- 2P%: 2 point percentage\n","- eFG%: effective field goal percentage\n","- FT: free throws\n","- FTA: free throws attempted\n","- FT%: free throw percentage\n","- ORB: offensive rebound\n","- DRB: defensive rebound\n","- TRB: total rebounds\n","- AST: assists\n","- STL: steals\n","- BLK: blocks\n","- TOV: turnovers\n","- PF: personal fouls\n","- PTS: points\n","\n","Load the dataset."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# answer goes here\n","\n","players = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Clustering/nba_player_seasons.csv')\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o_K1eQ0x1D_y"},"source":["The goal is to cluster these player-seasons to identify potential player \"archetypes\".  \n","The pre-processing steps will be identical to what we previously did for K-means.\n","\n","Begin by removing players whose season did not meet one of the following criteria:\n","1. Started at least 20 games\n","2. Averaged at least 10 minutes per game"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["players_df = players.loc[players['GS'] >= 20]\n","players_df = players.loc[players['MP'] >= 10]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"86lfEP_91D_0"},"source":["Choose a subset of numeric columns that is interesting to you from an \"archetypal\" standpoint. \n","\n","We will choose the following basic statistics: **points, total rebounds, assists, steals, blocks**, and **turnovers**, but you should feel free to choose other reasonable feature sets if you like. Be careful not to include too many dimensions (curse of dimensionality)."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PTS</th>\n      <th>TRB</th>\n      <th>AST</th>\n      <th>STL</th>\n      <th>BLK</th>\n      <th>TOV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.3</td>\n      <td>1.5</td>\n      <td>0.6</td>\n      <td>0.5</td>\n      <td>0.2</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.7</td>\n      <td>2.5</td>\n      <td>0.8</td>\n      <td>0.1</td>\n      <td>0.4</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.2</td>\n      <td>1.8</td>\n      <td>1.9</td>\n      <td>0.4</td>\n      <td>0.1</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13.9</td>\n      <td>9.5</td>\n      <td>1.6</td>\n      <td>1.5</td>\n      <td>1.0</td>\n      <td>1.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.9</td>\n      <td>7.3</td>\n      <td>2.2</td>\n      <td>0.9</td>\n      <td>0.8</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2134</th>\n      <td>8.4</td>\n      <td>2.6</td>\n      <td>2.5</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <th>2137</th>\n      <td>7.3</td>\n      <td>1.8</td>\n      <td>0.6</td>\n      <td>0.4</td>\n      <td>0.1</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>2138</th>\n      <td>15.1</td>\n      <td>9.0</td>\n      <td>1.8</td>\n      <td>1.5</td>\n      <td>0.5</td>\n      <td>1.9</td>\n    </tr>\n    <tr>\n      <th>2139</th>\n      <td>8.7</td>\n      <td>6.2</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>0.9</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>2140</th>\n      <td>6.1</td>\n      <td>3.0</td>\n      <td>0.5</td>\n      <td>0.2</td>\n      <td>0.4</td>\n      <td>0.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>1763 rows × 6 columns</p>\n</div>","text/plain":"       PTS  TRB  AST  STL  BLK  TOV\n0      5.3  1.5  0.6  0.5  0.2  0.5\n1      1.7  2.5  0.8  0.1  0.4  0.4\n2      3.2  1.8  1.9  0.4  0.1  0.8\n3     13.9  9.5  1.6  1.5  1.0  1.7\n4      8.9  7.3  2.2  0.9  0.8  1.5\n...    ...  ...  ...  ...  ...  ...\n2134   8.4  2.6  2.5  0.4  0.0  3.6\n2137   7.3  1.8  0.6  0.4  0.1  0.6\n2138  15.1  9.0  1.8  1.5  0.5  1.9\n2139   8.7  6.2  1.0  0.8  0.9  0.9\n2140   6.1  3.0  0.5  0.2  0.4  0.8\n\n[1763 rows x 6 columns]"},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["keep_cols = ['PTS', 'TRB', 'AST', 'STL', 'BLK', 'TOV']\n","\n","X = players_df[keep_cols]\n","\n","X"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MUjcLMMk1D_2"},"source":["Standardize the features in your dataset using scikit-learn's StandardScaler, which will set the mean of each feature to 0 and the variance to 1."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# answer goes here\n","\n","\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","\n","X_std = scaler.fit_transform(X)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ao3Y5FVp1D_4"},"source":["Run both K-Means and Gaussian mixtures modeling twice, once with 3 cluster and once with 7 clusters. Print out the resulting means for all 4 scenarios (KM+3, GMM+3, KM+7, GMM+7). When printing the means, transform the scaled versions back into their corresponding unscaled values. \n","\n","What \"archetypes\" do you see? Are the archetypes identified by GMM similar to those identified by K-Means? How do the means of GMM differ from those of K-Means?"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# answer goes here\n","result_means = {}\n","\n","for i in [3,7]:\n","    k_means = KMeans(n_clusters=i)\n","    k_means.fit(X_std)\n","    result_means['KM+{}'.format(i)] = pd.DataFrame(scaler.inverse_transform(k_means.cluster_centers_), columns=X.columns)\n","\n","    gmm = GaussianMixture(n_components=i)\n","    gmm.fit(X_std)\n","    result_means['GMM+{}'.format(i)] = pd.DataFrame(scaler.inverse_transform(gmm.means_), columns=X.columns)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":"{'KM+3':          PTS       TRB       AST       STL       BLK       TOV\n 0   6.772920  2.849381  1.498584  0.553805  0.283894  0.877522\n 1  17.829779  4.693382  5.204044  1.232353  0.417279  2.519853\n 2  12.535457  7.197507  1.783102  0.822715  0.981717  1.488920,\n 'GMM+3':          PTS       TRB       AST       STL       BLK       TOV\n 0  13.159741  3.542396  3.920065  0.991215  0.288711  1.872719\n 1   6.309249  2.900933  1.177581  0.486149  0.289141  0.772971\n 2  11.867051  6.746418  1.801008  0.815653  0.939441  1.441006,\n 'KM+7':          PTS       TRB       AST       STL       BLK       TOV\n 0  23.677778  8.369444  7.588889  1.605556  0.877778  3.808333\n 1   7.110438  4.738384  1.045455  0.523232  0.726599  0.888215\n 2  16.413158  9.764474  2.002632  0.813158  1.809211  1.957895\n 3  10.183820  2.891777  2.870027  0.842971  0.227851  1.403714\n 4   5.400666  2.313977  1.076872  0.427621  0.195840  0.684359\n 5  18.174342  4.415132  5.471711  1.284868  0.367105  2.538158\n 6  13.255357  6.660714  2.034821  0.950446  0.644643  1.511161,\n 'GMM+7':          PTS       TRB       AST       STL       BLK       TOV\n 0   5.474596  3.457486  0.763373  0.392423  0.433948  0.649332\n 1  11.710154  2.710703  3.509092  0.823483  0.175516  1.610253\n 2  17.251997  5.129121  5.664741  1.347297  0.450011  2.709301\n 3  16.526378  9.233323  2.704410  0.905083  1.532999  2.080224\n 4   6.445967  2.179109  1.342293  0.511194  0.183986  0.792322\n 5  12.132117  6.221522  1.572955  0.676627  0.717239  1.325308\n 6   8.823428  4.470955  2.040740  1.021115  0.490324  1.234501}"},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["result_means"]},{"cell_type":"markdown","metadata":{},"source":["##"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y0wBOUWJ1D_6"},"source":["Predict the likelihood of each season belonging to one of the 3 clusters using the GMM model. Then, calculate the entropy for each set of predicted probabilities. \n","\n","We will use entropy as a measure of how confident we are in the predicted class label. If we had no confidence in our prediction, we would assign 33% probability to each class, while if we were totally confident, we would assign 100% to one class. Entropy would be at a maximum in the \"no confidence\" scenario and a minimum in the \"full confidence\" scenario, which makes it a reasonable way to quantify our uncertainty in our prediction. There are certainly other methods as well; feel free to experiment with them if desired.\n","\n","Which five predicted labels are we least confident about? Which five are we most confident about? Print out the associated details (season, player name, stats, etc.) from those seasons."]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["# answer goes here\n","\n","gmm = GaussianMixture(n_components=3)\n","gmm.fit(X_std)\n","\n","probs = pd.DataFrame(gmm.predict_proba(X_std))\n","\n","entropies = pd.DataFrame(entr(gmm.predict_proba(X_std)))"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.059003</td>\n      <td>1.633222e-02</td>\n      <td>0.015970</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.015725</td>\n      <td>9.889136e-03</td>\n      <td>0.035869</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.131866</td>\n      <td>4.275594e-02</td>\n      <td>0.014039</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000158</td>\n      <td>8.777238e-06</td>\n      <td>0.000015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.006472</td>\n      <td>1.160051e-01</td>\n      <td>0.034731</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1758</th>\n      <td>0.000013</td>\n      <td>9.428235e-27</td>\n      <td>0.000149</td>\n    </tr>\n    <tr>\n      <th>1759</th>\n      <td>0.083947</td>\n      <td>2.418235e-02</td>\n      <td>0.014938</td>\n    </tr>\n    <tr>\n      <th>1760</th>\n      <td>0.001481</td>\n      <td>3.519259e-07</td>\n      <td>0.000171</td>\n    </tr>\n    <tr>\n      <th>1761</th>\n      <td>0.001226</td>\n      <td>3.543224e-01</td>\n      <td>0.337325</td>\n    </tr>\n    <tr>\n      <th>1762</th>\n      <td>0.016945</td>\n      <td>6.819301e-03</td>\n      <td>0.021827</td>\n    </tr>\n  </tbody>\n</table>\n<p>1763 rows × 3 columns</p>\n</div>","text/plain":"             0             1         2\n0     0.059003  1.633222e-02  0.015970\n1     0.015725  9.889136e-03  0.035869\n2     0.131866  4.275594e-02  0.014039\n3     0.000158  8.777238e-06  0.000015\n4     0.006472  1.160051e-01  0.034731\n...        ...           ...       ...\n1758  0.000013  9.428235e-27  0.000149\n1759  0.083947  2.418235e-02  0.014938\n1760  0.001481  3.519259e-07  0.000171\n1761  0.001226  3.543224e-01  0.337325\n1762  0.016945  6.819301e-03  0.021827\n\n[1763 rows x 3 columns]"},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["entropies"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.013768</td>\n      <td>9.835314e-01</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002650</td>\n      <td>9.900613e-01</td>\n      <td>0.007288</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.041413</td>\n      <td>9.562738e-01</td>\n      <td>0.002313</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000014</td>\n      <td>6.136264e-07</td>\n      <td>0.999985</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000927</td>\n      <td>3.443692e-02</td>\n      <td>0.964636</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1758</th>\n      <td>0.999987</td>\n      <td>1.471177e-28</td>\n      <td>0.000013</td>\n    </tr>\n    <tr>\n      <th>1759</th>\n      <td>0.021993</td>\n      <td>9.755154e-01</td>\n      <td>0.002492</td>\n    </tr>\n    <tr>\n      <th>1760</th>\n      <td>0.000171</td>\n      <td>1.984313e-08</td>\n      <td>0.999829</td>\n    </tr>\n    <tr>\n      <th>1761</th>\n      <td>0.000138</td>\n      <td>4.721763e-01</td>\n      <td>0.527686</td>\n    </tr>\n    <tr>\n      <th>1762</th>\n      <td>0.002900</td>\n      <td>9.931572e-01</td>\n      <td>0.003943</td>\n    </tr>\n  </tbody>\n</table>\n<p>1763 rows × 3 columns</p>\n</div>","text/plain":"             0             1         2\n0     0.013768  9.835314e-01  0.002700\n1     0.002650  9.900613e-01  0.007288\n2     0.041413  9.562738e-01  0.002313\n3     0.000014  6.136264e-07  0.999985\n4     0.000927  3.443692e-02  0.964636\n...        ...           ...       ...\n1758  0.999987  1.471177e-28  0.000013\n1759  0.021993  9.755154e-01  0.002492\n1760  0.000171  1.984313e-08  0.999829\n1761  0.000138  4.721763e-01  0.527686\n1762  0.002900  9.931572e-01  0.003943\n\n[1763 rows x 3 columns]"},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["probs"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["class_weights = {0: 1/3, 1: 1/3, 2: 1/3}\n","\n","entropies['weighted'] = entropies[class_weights.keys()].apply(\n","    lambda x: np.sum([x[i] * class_weights[i] for i in class_weights]), \n","    axis=1)\n","\n","most_certain_index = entropies.sort_values(by='weighted').index[0]\n","least_certain_index = entropies.sort_values(by='weighted', ascending=False).index[0]"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0    0.371660\n1    0.327138\n2    0.301202\nName: 1647, dtype: float64\n0     1.000000e+00\n1    1.317536e-221\n2    1.322268e-222\nName: 1292, dtype: float64\n"}],"source":["least_certain_prob = probs.iloc[least_certain_index]\n","most_certain_prob = probs.iloc[most_certain_index]\n","\n","print(least_certain_prob)\n","print(most_certain_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}